import os
import re
import requests
import json
from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
import google.generativeai as genai

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù†ÙˆØ§Ø© Ø§Ù„Ø³ÙŠØ§Ø¯ÙŠØ©
from dominator_brain import strategic_intelligence_core, alchemy_fusion_core, WPIL_DOMINATOR_SYSTEM

app = Flask(__name__, template_folder="templates", static_folder="static")
CORS(app)

# ========= ØªØ±Ø³Ø§Ù†Ø© Nebula Ù„Ø¹Ø§Ù… 2025 =========
MODELS_POOL = [
    "gemini-2.5-flash",
    "gemini-2.5-flash-lite",
    "gemini-2.0-flash-lite-001",
    "gemini-flash-latest"
]

genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
APIFY_KEY = os.getenv("APIFY_API_KEY")

def get_ai_response_nebula(prompt: str) -> str:
    for model_name in MODELS_POOL:
        try:
            model = genai.GenerativeModel(model_name)
            return model.generate_content(prompt).text
        except: continue
    return "ğŸš¨ ÙƒØ§ÙØ© Ø®Ø·ÙˆØ· Ø§Ù„Ø§ØªØµØ§Ù„ Ù…Ø´ØºÙˆÙ„Ø© Ø­Ø§Ù„ÙŠØ§Ù‹."

def fetch_live_dna(niche):
    """Ø¨Ø±ÙˆØªÙˆÙƒÙˆÙ„ Ø§Ù„Ø³Ø­Ø¨ Ø§Ù„Ø­ÙŠ Ø§Ù„Ù…Ø·ÙˆØ± Ù„Ø§Ø³Øªf"https://twitter.com/{user_name}/status/{tweet_id}" if tweet_id else None)
                        
                        results.append({
                            "text": i.get("full_text") or i.get("text") or "DNA Sample",
                            "engagement": f"{i.get('favorite_count', 0)}",
                            "author": user_name,
                            "url": actual_url, # Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ
                            "is_live": True,
                            "score": 85 + (int(i.get('favorite_count', 0)) % 15)
                        })
                    return results
        except Exception as e:
            print(f"Apify Error: {e}")
            
    # Ø¥Ø°Ø§ Ù„Ù… ÙŠØ¬Ø¯ Ø±ÙˆØ§Ø¨Ø· Ø­Ù‚ÙŠÙ‚ÙŠØ©ØŒ ÙŠØ¹Ø·ÙŠ Ø¬ÙŠÙ†Ø§Øª Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© ÙˆØ§Ø¶Ø­Ø©
    return [
        {
            "text": f"Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø³ÙŠØ§Ø¯Ø© ÙÙŠ {niche} Ù„Ø¹Ø§Ù… 2026", 
            "engagement": "Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ", 
            "author": "Dominator_AI", 
            "url": f"https://twitter.com/search?q={niche}&f=live",
            "is_live": False,
            "score": 98
        }
    ]

@app.route("/")
def home(): return render_template("index.html")

@app.route("/alchemy/discover", methods=["POST"])
def discover():
    niche = request.get_json().get("niche", "Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©")
    posts = fetch_live_dna(niche)
    fusion = alchemy_fusion_core(posts, niche)
    output = get_ai_response_nebula(f"{WPIL_DOMINATOR_SYSTEM}\n{fusion['synthesis_task']}")
    return jsonify({"super_post": output, "sources": posts}), 200

@app.route("/generate_all", methods=["POST"])
def generate():
    idea = request.getØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©"""
    if APIFY_KEY:
        try:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Actor Ø§Ù„Ø³Ø­Ø¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ Ù…Ù‡Ù„Ø© Ø£Ø·ÙˆÙ„ Ù‚Ù„ÙŠÙ„Ø§Ù‹ Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø¯Ù‚Ø©
            url = f"https://api.apify.com/v2/acts/apidojo~tweet-scraper/run-sync-get-dataset-items?token={APIFY_KEY}"
            payload = {
                "searchTerms": [niche],
                "maxTweets": 5,
                "searchMode": "top",
                "addUserInfo": True
            }
            res = requests.post(url, json=payload, timeout=40)
            if res.status_code in [200, 201]:
                data = res.json()
                if data and len(data) > 0:
                    refined_posts = []
                    for i in data:
                        text = i.get("full_text") or i.get("text")
                        if not text: continue
                        
                        # Ø¨Ù†Ø§Ø¡ Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù†Ø´ÙˆØ± Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ù‚Ø³Ø±ÙŠØ§Ù‹
                        username = i.get("user", {}).get("screen_name") or "user"
                        tweet_id = i.get("id_str") or i.get("id")
                        direct_url = f"https://x.com/{username}/status/{tweet_id}" if tweet_id else f"https://x.com/search?q={niche}"
                        
                        refined_posts.append({
                            "text": text,
                            "engagement": f"{int(i.get('favorite_count', 0)) + int(i.get('retweet_count', 0))}",
                            "author": username,
                            "url": direct_url,
                            "score": 85 + (int(i.get('favorite_count', 0)) % 15)
                        })
                    if refined_posts: return refined_posts_json().get("text", "Ø§Ù„Ø³ÙŠØ§Ø¯Ø©")
    prompt = f"{WPIL_DOMINATOR_SYSTEM}\nØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ù„Ù€ [LINKEDIN], [TWITTER], [TIKTOK] Ù„Ù„ÙÙƒØ±Ø©: {idea}"
    raw = get_ai_response_nebula(prompt)
    brain = strategic_intelligence_core(idea)
    
    parts = {"linkedin": "", "twitter": "", "tiktok": ""}
    for p in parts:
        match = re.search(rf"\[{p.upper()}\](.*?)(\[|$)", raw, re.S | re.I)
        parts[p] = match.group(1).strip() if match else "ÙØ´Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù‚Ø³Ù…"
        
    return jsonify({**parts, "video_prompt": brain["video_segments"]}), 200

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=int(os.getenv("PORT", 5000)))
